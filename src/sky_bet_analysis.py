# -*- coding: utf-8 -*-
"""sky_bet_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rl-uw9S-JfW1z9fUedy19AyTn7V0XwtB

Connecting Drive to Colab
Mounting Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""Setting up PySpark in Colab"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

"""Installing Apache Spark 3.1.2 with Hadoop 3.2 from the link"""

!wget -q https://downloads.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz

"""To unzip that folder"""

!tar xf spark-3.1.2-bin-hadoop3.2.tgz

"""Install findspark library"""

!pip install -q findspark

"""To set the environment path"""

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop3.2"

"""To locate Spark in the system"""

import findspark
findspark.init()

"""To know the location where Spark is installed"""

findspark.find()

"""To view the Spark UI"""

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip
get_ipython().system_raw('./ngrok http 4050 &')
!curl -s http://localhost:4040/api/tunnels

from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local[*]")\
        .appName("League Analysis")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()
print("A Sky Bet League Competition Analysis")

"""Read Sky_competition_clean.txt csv file into PySpark"""

match_data = spark.read.csv("/content/Sky_competition_clean.txt", header=True, inferSchema=True)

"""Understanding the Data"""

match_data.printSchema()

"""To see the physical plan """

match_data.explain()

"""Show top 5 rows"""

match_data.show(5)

"""Drop the last column ('_c18')"""

#match_data_clean = match_data.drop('_c18').show(10)

"""Add new column (team_name)"""

match_data = match_data.withColumn("team_name", match_data.away_team_name)

match_data.show(5)

"""Add new column (total_match_score)"""

match_data = match_data.withColumn("total_match_score", match_data.away_team_score + match_data.home_team_score)

match_data.show(5)

"""Create a view"""

match_data.createOrReplaceTempView("match_data")

"""Total number of teams in the league"""

spark.sql("SELECT COUNT(DISTINCT(team_name)) AS total_number_of_teams FROM match_data").show()

"""Team with the highest average number of goals per match"""

spark.sql("SELECT home_team_id, home_team_name, \
            AVG(home_team_score) as average_home_team_score, away_team_id, away_team_name, \
            AVG(away_team_score) as average_away_team_score FROM match_data \
            GROUP BY home_team_id, home_team_name, away_team_id, away_team_name \
            ORDER BY average_home_team_score DESC").show()

"""Most common match score"""

spark.sql("SELECT away_team_score, \
            COUNT(away_team_score) AS away_team_value_occurrence, \
            home_team_score, \
            COUNT(away_team_score) AS home_team_value_occurrence \
            FROM match_data \
            GROUP BY away_team_score, home_team_score \
            ORDER BY away_team_value_occurrence DESC, home_team_value_occurrence DESC \
            LIMIT 1").show()

"""Avenue with the highest number of goals"""

spark.sql("SELECT match_venue_id, match_venue_name, \
            COUNT(total_match_score) as highest_venue_score FROM match_data \
            GROUP BY match_venue_id, match_venue_name ORDER BY highest_venue_score DESC").show()

# spark.sql("SELECT away_team_id, home_team_id, sum(cast((match_data.away_team_score + match_data.home_team_score) as BIGINT)) \
#             from match_data where away_team_id == home_team_id group by away_team_id, home_team_id").show()

"""Number of matches played both home and away"""

spark.sql("SELECT team_name, COUNT(away_team_id) AS away_matches_played, \
          COUNT(home_team_id) AS home_matches_played \
          FROM match_data \
          GROUP BY team_name").show()

"""Points from a match"""

spark.sql("SELECT team_name, away_team_score, \
          home_team_score, \
          CASE \
            WHEN away_team_score > home_team_score THEN '3 points away' \
            WHEN away_team_score == home_team_score THEN 'Draw' \
            ELSE '3 points home' \
          END AS Points \
          FROM match_data \
          GROUP BY team_name, away_team_score, home_team_score").show()

"""Total Number of Goals Scored by Teams"""

spark.sql("SELECT away_team_id, team_name, sum(away_team_score) as total_goal_scored\
          FROM match_data as a \
          where a.away_team_name = a.team_name \
          GROUP BY away_team_id, team_name \
          ORDER BY total_goal_scored DESC").show()

"""Save to file"""

match_data.write.csv("/content/drive/MyDrive/sky_bet_competition/processed_data.csv", header=True)

"""Why I decided to use Pyspark libraries:

1.   I prefer using a distributed approach whenever I am doing data analysis.
2.   Last but not the least, the compute and storage capabilities are more advantageous.




"""